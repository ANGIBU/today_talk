import requests
from bs4 import BeautifulSoup
from datetime import datetime
from services.news_service import save_news_to_db

# ë„¤ì´ë²„ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ URL ëª©ë¡
NAVER_NEWS_CATEGORIES = {
    "politics": "https://news.naver.com/section/100",
    "economy": "https://news.naver.com/section/101",
    "domestic": "https://news.naver.com/section/102",
    "world": "https://news.naver.com/section/104",
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def get_article_content(url):
    """ê¸°ì‚¬ ìƒì„¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        # URLì´ /n.news.naver.comìœ¼ë¡œ ì‹œì‘í•˜ë©´ https://ë¥¼ ì¶”ê°€
        if url.startswith("/n.news.naver.com"):
            url = f"https:{url}"
            
        response = requests.get(url, headers=HEADERS, timeout=5)
        if response.status_code != 200:
            print(f"âš ï¸ ê¸°ì‚¬ ì ‘ê·¼ ì‹¤íŒ¨: {response.status_code}")
            return None

        soup = BeautifulSoup(response.text, "html.parser")
        
        # ê¸°ë³¸ ë©”íƒ€ ì •ë³´
        category_tag = soup.select_one(".media_end_categorize_item")
        date_tag = soup.select_one(".media_end_head_info_datestamp_time")
        source_tag = soup.select_one(".media_end_head_top_logo_text")
        author_tag = soup.select_one(".media_end_head_journalist_name")
        author_email = soup.select_one(".byline_s")
        
        # ê¸°ì‚¬ ìš”ì•½
        summary = soup.select_one(".media_end_summary")
        summary_text = summary.get_text(strip=True) if summary else ""
        
        # ë³¸ë¬¸ ë‚´ìš©
        content_area = soup.select_one("#dic_area")
        if not content_area:
            print(f"âš ï¸ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
            return None
            
        # ì´ë¯¸ì§€ ì •ë³´ ìˆ˜ì§‘
        images = []
        for img_div in content_area.select(".end_photo_org"):
            img = img_div.select_one("img")
            if img:
                caption = img_div.select_one(".img_desc")
                images.append({
                    "url": img.get("src", ""),
                    "alt": img.get("alt", ""),
                    "caption": caption.get_text(strip=True) if caption else None
                })
        
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        content_text = []
        for element in content_area.contents:
            if element.name == "span" and "end_photo_org" in element.get("class", []):
                continue  # ì´ë¯¸ì§€ ê±´ë„ˆë›°ê¸°
            elif element.name == "br":
                content_text.append("\n")
            elif isinstance(element, str):
                text = element.strip()
                if text:
                    content_text.append(text)
            elif element.name is None:
                text = element.strip()
                if text:
                    content_text.append(text)
                    
        content_text = " ".join(content_text).strip()
        
        # ì›ë¬¸ URLê³¼ ì €ì‘ê¶Œ
        original_url = soup.select_one(".media_end_head_origin_link")
        copyright = soup.select_one(".copyright .c_text")
        
        return {
            "category": category_tag.get_text(strip=True) if category_tag else None,
            "published_at": date_tag.get("data-date-time") if date_tag else None,
            "source": source_tag.get_text(strip=True) if source_tag else None,
            "author": author_tag.get_text(strip=True) if author_tag else None,
            "author_email": author_email.get_text(strip=True) if author_email else None,
            "summary": summary_text,
            "content": content_text,
            "images": images,
            "original_url": original_url["href"] if original_url else None,
            "copyright": copyright.get_text(strip=True) if copyright else None
        }
        
    except Exception as e:
        print(f"âš ï¸ ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None

def scrape_naver_news(category_url, category):
    """ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜"""
    print(f"ğŸ” {category} ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {datetime.now()}")
    news_list = []

    try:
        response = requests.get(category_url, headers=HEADERS, timeout=5)
        if response.status_code != 200:
            print(f"âŒ {category} ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {response.status_code}")
            return news_list

        soup = BeautifulSoup(response.text, "html.parser")
        articles = soup.select("li.sa_item")  # ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡

        for article in articles:
            try:
                title_tag = article.select_one("a.sa_text_title")
                link_tag = article.select_one("a.sa_text_title")
                press_tag = article.select_one("div.sa_text_press")
                
                if not title_tag or not link_tag:
                    continue

                link = link_tag["href"] if link_tag.has_attr("href") else ""
                
                # ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                article_detail = get_article_content(link)
                if not article_detail:
                    continue

                news_item = {
                    "category": category,
                    "title": title_tag.get_text(strip=True),
                    "source_url": link,
                    "source": press_tag.get_text(strip=True) if press_tag else "ì¶œì²˜ ì—†ìŒ",
                    "published_at": datetime.now(),
                    "content": article_detail["content"],
                    "images": article_detail["images"],
                    "author": article_detail["author"],
                    "author_email": article_detail["author_email"],
                    "original_url": article_detail["original_url"],
                    "copyright": article_detail["copyright"],
                    "views": 0
                }
                news_list.append(news_item)

            except Exception as e:
                print(f"âš ï¸ {category} ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

        # DB ì €ì¥
        save_news_to_db(news_list)
        print(f"âœ… {category} ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ {category} ë‰´ìŠ¤ ìš”ì²­ ì‹¤íŒ¨: {e}")

    return news_list

def main():
    """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
    # ê° ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘
    for category, url in NAVER_NEWS_CATEGORIES.items():
        try:
            scrape_naver_news(url, category)
        except Exception as e:
            print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()